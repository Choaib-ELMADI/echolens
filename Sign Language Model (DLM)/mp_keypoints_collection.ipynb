{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f635e8b4",
   "metadata": {},
   "source": [
    "# 1. Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc2a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9efe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_background = (198, 63, 88)  # PURPLE\n",
    "corner_color = (53, 53, 249)     # RED\n",
    "text_color = (239, 239, 239)     # WHITE\n",
    "border_color = (61, 147, 8)      # GREEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b78c38",
   "metadata": {},
   "source": [
    "# 5. Setup Folders for Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fdc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"Data\")\n",
    "actions = np.array([\"You\", \"Yes\", \"WhatAreYouDoing\", \"TryBeing\", \"ToMeet\", \"ThankYou\", \"TakeCare\", \"SameAsYou\", \"Question\", \"Point\", \"Nothing\", \"IHear\", \"HowAreYou\", \"Hello\", \"Bye\", \"Good\", \"Busy\", \"_BLANK\"])\n",
    "no_sequences = 20\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0685f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3f8f1",
   "metadata": {},
   "source": [
    "# 6. Collect MP Keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5041d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "\n",
    "def mediapipe_detections(frame, model):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    results = model.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return frame, results\n",
    "\n",
    "\n",
    "def draw_landmarks(frame, results, color):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    hand_landmarks = np.zeros(63)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = np.array(\n",
    "            [\n",
    "                [landmark.x, landmark.y, landmark.z]\n",
    "                for landmark in results.multi_hand_landmarks[0].landmark\n",
    "            ]\n",
    "        ).flatten()\n",
    "\n",
    "    return hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "385ccc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "\n",
    "stream_url = \"http://192.168.169.196:81/stream\"\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "no_frames_counter = 0\n",
    "no_sequences_counter = 0\n",
    "index = 12                        # Curr Action  : HowAreYou\n",
    "current_action = actions[index]   # Done Actions : You, Yes, WhatAreYouDoing, TryBeing, ToMeet, ThankYou, TakeCare, SameAsYou, Question, Point, Nothing, IHear, \n",
    "                                  # Correction   : WhatAreYouDoing, ToMeet, ThankYou, SameAsYou, IHear\n",
    "\n",
    "while True:\n",
    "    _, image = cap.read()\n",
    "    \n",
    "    image, results = mediapipe_detections(image, hands)\n",
    "    draw_landmarks(image, results, corner_color)\n",
    "    right_hand = extract_keypoints(results)\n",
    "    # print(hand_landmarks)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord(\"s\"):\n",
    "        npy_path = os.path.join(DATA_PATH, current_action, str(no_sequences_counter), f\"{no_frames_counter}.npy\")\n",
    "        np.save(npy_path, right_hand)\n",
    "        no_frames_counter += 1\n",
    "        if no_frames_counter == sequence_length:\n",
    "            no_frames_counter = 0\n",
    "            no_sequences_counter += 1\n",
    "            if no_sequences_counter == no_sequences:\n",
    "                break\n",
    "    \n",
    "    cv2.putText(image, f\"Collecting Frames for '{current_action}'\", (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(image, f\"Video Num: {no_sequences_counter}\", (15, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(image, f\"Frame Num: {no_frames_counter}\", (15, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    \n",
    "    if key == ord(\"n\"):\n",
    "        cv2.imwrite(f\"{current_action}-image-{no_frames_counter}-{no_sequences_counter}.jpg\", image)\n",
    "    \n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cd9d0",
   "metadata": {},
   "source": [
    "# 7. Preprocess Data and Create Labels and Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b20a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e966b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4406d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c8ca1d",
   "metadata": {},
   "source": [
    "# 8. Build and Train LSTM Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d62b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcdc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir) # for moritoring the NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation=\"relu\", input_shape=(5, 63)))\n",
    "model.add(LSTM(128, return_sequences=True, activation=\"relu\"))\n",
    "model.add(LSTM(64, return_sequences=False, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [.7, .3]\n",
    "actions[np.argmax(res)]\n",
    "actions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d95357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=1000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"action.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c064bb",
   "metadata": {},
   "source": [
    "# 9. Make Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508155a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(results[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed148b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e54d2",
   "metadata": {},
   "source": [
    "# 10. Save Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"action.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a5c16",
   "metadata": {},
   "source": [
    "# 11. Evaluation using Confusion Matrix and Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a69814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d188956",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a83f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bf107",
   "metadata": {},
   "source": [
    "# 12. Test Real Time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.7\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    frame, results = mediapipe_detections(frame, holistic)\n",
    "    draw_landmarks(frame, results, corner_color)\n",
    "\n",
    "    right_hand = extract_keypoints(results)\n",
    "    sequence.append(right_hand)\n",
    "    sequence = sequence[-5:]\n",
    "    \n",
    "    if len(sequence) == 5:\n",
    "        res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        predictions.append(np.argmax(res))\n",
    "    \n",
    "        if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                        print(\" \".join(sentence))\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            \n",
    "    frame = prob_viz(res, actions, frame, colors)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SignLangVenvDLM",
   "language": "python",
   "name": "signlangvenvdlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
